{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Open Source Intelligence Pipeline üê¶üîé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to create a data pipeline for intel analyst, looking <br>\n",
    "to expedite the use of open source twitter data. The specific use case that is being <br>\n",
    "presented here, is to collect open source images of maritime vessels in an effort <br>\n",
    "to verify the position of vessels based on an image being capture while they are <br>\n",
    "close to shore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This pipeline achomplishes:\n",
    "    - Twitter data collection based on hashtag\n",
    "    - Downloading of images related to collected tweets\n",
    "    - Verification and classification of boats in images\n",
    "    - Cleaning of the dataset to only, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let start by setting up the enviroment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_req():\n",
    "    os.chdir('..')\n",
    "    path = os.getcwd() + '/requirements.txt'\n",
    "    cmd_dl = 'pip install -r ' + path\n",
    "    os.system(cmd_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "install_req()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Continued, importing all the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = os.getcwd()\n",
    "sys.path.append(path) #Set the system path for imports\n",
    "from twitter.twitter_scrape import * # Scraping tool\n",
    "from postgree.df_sql_utils import * # Postgress utilities\n",
    "from postgree.df_sql_utils import main as sql_main # Sql main function\n",
    "from twitter.twitter_scrape import main as tws_main # Scraping main\n",
    "from aws_custom.aws_file_upload import main as aws_up_main # AWS fileuploader\n",
    "from aws_custom.aws_config import * #AWS Config\n",
    "from aws_custom.aws_model_utils import * # Utils for running inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd() \n",
    "path_down = path + '/download/'\n",
    "path_up = path + '/upload/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Postgree Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = conn # Establishing a connection to the postgrees server\n",
    "create_db(conn) #If the database isnt created, create one\n",
    "create_table() #If the table isnt created, create one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Twitter for Tweet with specific Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tws_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tws_main function we are using the tweepy twitter api tool to query twitters <br>\n",
    "recent tweets under the common accounts and hashtags that intel analysts use <br>\n",
    "more specifically we are searching the hashtags 'Shipping', 'Shipsinpics', and 'Ship'. <br>\n",
    "There is a large amount of different data, but the data of interest is the id, text, <br>\n",
    "tweet creation time, and the url of the image related to the tweet. Additionally <br>\n",
    "this function provides functionalility to automatically filter tweets that do not <br>\n",
    "contain images. Lastly the the tweets are packaged into a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the images associated with Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_image(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The free twitter api does not contain the functionality to download images related <br>\n",
    "to tweets, this function takes care of this by downloading all the images in our tweet dataframe. These tweets are saved in the downloads folder, named with the id of the <br>\n",
    "tweets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter for tweets that only pertain to boats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list_images() # List path to all the images\n",
    "delete_list = check_images(image_files, END_POINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the validity of the tweets that were collected we are going to send the <br>\n",
    "collected images against the boat classifcation model, if the image does has a 'non boat' label score greater than 80% then it is marked to be removed from the directory of images, and its <br> associated tweet. <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move all Images that contain boats, and create new data frame with only viable tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = remove_from_df(df, delete_list)\n",
    "delete_non_boats(delete_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step of the pipeline the images, and tweets that are unrelated to boats are removed. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Images to S3 Bucket for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_for_upload(path_down,path_up)\n",
    "aws_up_main(path_up,'twitter-osint') \n",
    "delete_temp_folders(path_down, path_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the end of this pipeline is to feed the collected boat information into a GUI for analyst <br> we are going to upload all the images of boats the an AWS S3 Bucket. This uses the AWS CLI <br> to properally connect to AWS. Lastly in this step we are cleaning up the temporary folders <br> that were used in the download and upload process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Tweets into a Postgrees Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_main(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly the tweets are formatted to be intserted into a postgrees database, where they can be <br>\n",
    "retrieved by tweet id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- APIs come with an extreme amount of over head for set up and learning. Some of them can <br>\n",
    "essentially be like learning a new language.\n",
    "- If there is something that you are wanting to do with python there is likely a package for it <br>\n",
    "- Although a workflow sounds incredibly simple at first, the problem is always more complex than you would expect. <br>\n",
    "- Before starting to code, it is always helpful to start with a literature review of the major <br> tools that you want use. I wish we would have done this with the Tweepy, the python <br>\n",
    "package for the Twitter API.\n",
    "- The process of making code generalizable, where it isnt overly dependent on just being run <br> on your machine is alot of work. \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5222b9e6c71d7f1e2bb61caa89897cbb7b34f97b55cd9b91d5800888aee1f8a7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
